{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Web Scraping using Python.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1DxCNCObO_d",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraping using Python\n",
        "\n",
        "Whenever we start a Data Science project, the first thing we require is a dataset to work on. While there are many sources where datasets are available, we might want to create a dataset using the data found on a website.\n",
        "\n",
        "In this notebook, we'll  explore the process to extract information from Wikipedia and form a dataset which can later be used for Data Analytics and Machine Learning applications.\n",
        "\n",
        "Use link : https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUQq9_OJbO_i",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "We'll first import all relevant libraries that we will require to access a website's HTML and extract information from the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaqzRkaTbO_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W2-bM7ybO_z",
        "colab_type": "text"
      },
      "source": [
        "## Define functions\n",
        "\n",
        "Firstyly, we define the function getHTMLContent, that accepts a url and uses BeautifulSoup library to get the HTML for a webpage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAmNX96XbO_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEcXFFslbO_8",
        "colab_type": "text"
      },
      "source": [
        "## Understand the data\n",
        "\n",
        "The webpage includes the information we need in the form of HTML table. Thus, we need to reach that table and extract the information. However, there might be multiple tables on the page. We would thus need to find the class of that table and then access its data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zbUHJBYbO__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use this code\n",
        "content = getHTMLContent('https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population')\n",
        "tables = content.find_all('table')\n",
        "for table in tables:\n",
        "    print(table.prettify())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fws9ennVbPAH",
        "colab_type": "text"
      },
      "source": [
        "The table that we will use has the class 'wikitable sortable'. It has rows of information where the first row has headings and the other rows in succession have information about each country.\n",
        "\n",
        "Next, we explore the website for each country.\n",
        "Print all the links of the country."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFsfti17bPAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4uwj67BbPAQ",
        "colab_type": "text"
      },
      "source": [
        "Each row has a link to the corresponding country page on Wikipedia. However, the initial weblink is missing, so we would have to append it. Let's understand the content of page with the example of one page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ro2kUWJbPAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pvhW9rxbPAY",
        "colab_type": "text"
      },
      "source": [
        "## Create the dataset\n",
        "\n",
        "Now that we have identified what all information needs to be extracted and how. We have compiled the whole process as a function above. Now, we just move across each row of the Country list and compile its data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8qKAmV6bPAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use this code\n",
        "data_content = []\n",
        "for row in rows:\n",
        "    cells = row.find_all('td')\n",
        "    if len(cells) > 1:\n",
        "        print(cells[1].get_text())\n",
        "        country_link = cells[1].find('a')\n",
        "        country_info = [cell.text.strip('\\n') for cell in cells]\n",
        "        additional_details = getAdditionalDetails(country_link.get('href'))\n",
        "        if (len(additional_details) == 4):\n",
        "            country_info += additional_details\n",
        "            data_content.append(country_info)\n",
        "\n",
        "dataset = pd.DataFrame(data_content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NijOLQunbPAh",
        "colab_type": "text"
      },
      "source": [
        "Now, our dataset is compiled together but lacks headers for columns. Thus, we would now add those headers and remove columns that bring no value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3CNS7YrbPAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use this code\n",
        "# Define column headings\n",
        "headers = rows[0].find_all('th')\n",
        "headers = [header.get_text().strip('\\n') for header in headers]\n",
        "headers += ['Total Area', 'Percentage Water', 'Total Nominal GDP', 'Per Capita GDP']\n",
        "dataset.columns = headers\n",
        "\n",
        "drop_columns = ['Rank', 'Date', 'Source']\n",
        "dataset.drop(drop_columns, axis = 1, inplace = True)\n",
        "dataset.sample(3)\n",
        "\n",
        "dataset.to_csv(\"Dataset.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}